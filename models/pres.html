<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fine-tuned Large Language Models for PNRR-FAIR</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/theme/white.css">
    <style>
        .reveal h1, .reveal h2, .reveal h3 {
            text-transform: none;
        }
        .reveal section img {
            border: none;
            box-shadow: 0 0 10px rgba(0,0,0,0.15);
        }
        .reveal .small-text {
            font-size: 0.7em;
        }
        .reveal .smaller-text {
            font-size: 0.6em;
        }
        .reveal ul, .reveal ol {
            font-size: 0.85em;
        }
        .reveal code {
            background: #f4f4f4;
            padding: 2px 6px;
            border-radius: 3px;
        }
        .footer {
            position: absolute;
            bottom: 10px;
            left: 10px;
            font-size: 0.5em;
            color: #666;
        }
    </style>
</head>
<body>
    <div class="reveal">
        <div class="slides">
            
            <!-- Title Slide -->
            <section>
                <h1>Fine-tuned Large Language Models</h1>
                <h3>for the PNRR-FAIR project</h3>
                <p class="small-text">Technical report v. 2.1</p>
                <p class="smaller-text">Prepared by G. Monti and A. Provetti<br>
                Network Centrality Labs and Birkbeck, University of London</p>
            </section>

            <!-- Premise -->
            <section>
                <h2>Premise</h2>
                <p class="small-text">Generative AI is transforming the media landscape. Tools capable of producing synthetic text, images, video, and audio are rapidly diffusing across newsrooms, social platforms, and search engines.</p>
                <p class="small-text">These outputs fuel innovation and creativity, but also trigger profound uncertainty around distinguishing:</p>
                <ul class="smaller-text">
                    <li>Truth vs. fabrication</li>
                    <li>Reporting vs. fiction</li>
                    <li>Evidence vs. illusion</li>
                </ul>
            </section>

            <!-- Overview -->
            <section>
                <h2>Project Overview</h2>
                <p class="small-text">Three bespoke Generative models trained on openly-accessible datasets:</p>
                <ol class="small-text">
                    <li><strong>Image Generation:</strong> Historical jewellery from V&A Museum</li>
                    <li><strong>Narrative:</strong> British (A. Christie) and American (H.P. Lovecraft) literary styles</li>
                    <li><strong>News Articles:</strong> Contemporary British news (The Guardian)</li>
                </ol>
            </section>

            <!-- Part 1: Jewellery Generation -->
            <section>
                <section>
                    <h2>Part 1: Museum Jewellery Generation</h2>
                    <p class="small-text">Full pipeline for domain-specific image generation</p>
                </section>

                <section>
                    <h3>1. Data Acquisition - V&A Museum API</h3>
                    <ul class="smaller-text">
                        <li>Programmatic harvesting of museum-quality imagery</li>
                        <li>Keyword query: "Coronet" in jewellery domain</li>
                        <li>IIIF (International Image Interoperability Framework) for high-resolution access</li>
                        <li>Up to 2500px resolution downloads</li>
                    </ul>
                    <p class="smaller-text"><strong>Metadata fields:</strong></p>
                    <ul class="smaller-text">
                        <li><code>systemNumber</code> (unique identifier)</li>
                        <li><code>_primaryTitle</code>, <code>_summary</code></li>
                        <li><code>_iiif_image_base_url</code> (image endpoint)</li>
                    </ul>
                </section>

                <section>
                    <h3>2. Automated Caption Generation - GPT-4.1</h3>
                    <p class="small-text">Uniform, descriptive captions for training consistency</p>
                    <ul class="smaller-text">
                        <li>Base64-encoded images → GPT-4.1 chat.completions</li>
                        <li>Structured annotations:
                            <ul>
                                <li>Material composition</li>
                                <li>Craftsmanship details</li>
                                <li>Gemstone types and settings</li>
                                <li>Stylistic and historical cues</li>
                                <li>Physical condition</li>
                            </ul>
                        </li>
                        <li>Output: <code>metadata_gpt4.jsonl</code> with harmonised captions</li>
                    </ul>
                </section>

                <section>
                    <h3>3. Model Training - DreamBooth LoRA</h3>
                    <p class="small-text">LoRA (Low-Rank Adaptation) for efficient fine-tuning</p>
                    <div style="display: flex; justify-content: space-between;">
                        <div style="width: 48%;">
                            <p class="smaller-text"><strong>Configuration:</strong></p>
                            <ul class="smaller-text">
                                <li>Base: <code>stable-diffusion-xl-base-1.0</code></li>
                                <li>Resolution: 512×512</li>
                                <li>FP16 mixed precision</li>
                                <li>SNR γ = 5.0</li>
                                <li>2000 training steps</li>
                                <li>8-bit Adam optimizer</li>
                            </ul>
                        </div>
                        <div style="width: 48%;">
                            <p class="smaller-text"><strong>Learned features:</strong></p>
                            <ul class="smaller-text">
                                <li>Metallic reflections</li>
                                <li>Stone translucency</li>
                                <li>Casting irregularities</li>
                                <li>Patina and age</li>
                                <li>Museum lighting</li>
                                <li>Catalogue photography style</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h3>Results: Before and After Fine-tuning</h3>
                    <p class="smaller-text"><strong>Same prompt:</strong> "A high-quality photo of a real historical TOK bracelet crafted in engraved silver with interlocking geometric panels and a central oval red gemstone..."</p>
                    <div style="display: flex; justify-content: space-around; margin-top: 20px;">
                        <div style="text-align: center; width: 45%;">
                            <p class="smaller-text"><strong>Before (Base SDXL)</strong></p>
                            <div style="background: #f0f0f0; height: 200px; display: flex; align-items: center; justify-content: center; border: 2px solid #ccc;">
                                <p class="smaller-text">Image 1: Inconsistent texture,<br>distorted shape,<br>watermark artifacts</p>
                            </div>
                        </div>
                        <div style="text-align: center; width: 45%;">
                            <p class="smaller-text"><strong>After (LoRA Fine-tuned)</strong></p>
                            <div style="background: #f0f0f0; height: 200px; display: flex; align-items: center; justify-content: center; border: 2px solid #ccc;">
                                <p class="smaller-text">Image 2: Sharp engraving,<br>realistic gemstone,<br>museum-quality lighting</p>
                            </div>
                        </div>
                    </div>
                    <p class="smaller-text" style="margin-top: 15px;"><em>Note: "TOK" is a fictional trigger word for the fine-tuned model</em></p>
                </section>

                <section>
                    <h3>Additional Example</h3>
                    <p class="smaller-text"><strong>Complex prompt:</strong> "A high-quality photo of a TOK real historical opal necklace featuring a pendant in a golden metal with a central blue stone..."</p>
                    <div style="background: #f0f0f0; height: 300px; display: flex; align-items: center; justify-content: center; border: 2px solid #ccc; margin: 20px auto;">
                        <p class="small-text">Generated opal necklace<br>with museum-quality rendering</p>
                    </div>
                </section>

                <section>
                    <h3>Resources - Jewellery Model</h3>
                    <ul class="small-text">
                        <li><strong>Training Code:</strong><br>
                        <a href="https://colab.research.google.com/drive/1HcHKwGRtTga8PtJewQX_YUUUc0QhOWDD?usp=sharing" style="font-size: 0.7em;">Google Colab Notebook</a></li>
                        <li><strong>Model Weights:</strong><br>
                        <a href="https://drive.google.com/drive/folders/1bZtsh8awCpYiIGpz3AL2atPGoXgCLRin" style="font-size: 0.7em;">Google Drive</a> or HuggingFace.co</li>
                    </ul>
                </section>
            </section>

            <!-- Part 2: Narrative Generation -->
            <section>
                <section>
                    <h2>Part 2: Mixture-of-Experts<br>Narrative Generation</h2>
                    <p class="small-text">Lightweight MoE architecture using two LoRA-fine-tuned Gemma-2B models</p>
                </section>

                <section>
                    <h3>Technical Motivation</h3>
                    <p class="small-text">Traditional LLMs produce <em>blended</em> stylistic approximations</p>
                    <p class="small-text"><strong>Solution:</strong> Parameter-efficient fine-tuning (PEFT) with LoRA</p>
                    <ul class="smaller-text">
                        <li>Small, targeted matrices modify tiny subset of parameters</li>
                        <li>Two separate adapters = true stylistic decoupling</li>
                        <li>No catastrophic forgetting</li>
                        <li>Independent training, dynamic loading</li>
                    </ul>
                </section>

                <section>
                    <h3>Model Architecture</h3>
                    <ul class="small-text">
                        <li><strong>Base model:</strong> <code>google/gemma-2-2b-it</code></li>
                        <li><strong>Fine-tuning:</strong> LoRA adapters</li>
                        <li><strong>Training:</strong> Hugging Face AutoTrain Advanced</li>
                        <li><strong>Task:</strong> Causal language modeling</li>
                        <li><strong>Precision:</strong> FP16 inference, INT4 quantization</li>
                        <li><strong>Context length:</strong> 1024 tokens</li>
                    </ul>
                    <p class="small-text" style="margin-top: 20px;"><strong>Each LoRA adapter:</strong></p>
                    <ul class="smaller-text">
                        <li>Under 300MB</li>
                        <li>Fast to load and swap</li>
                        <li>Stackable on base model</li>
                    </ul>
                </section>

                <section>
                    <h3>Expert 1: H.P. Lovecraft Model</h3>
                    <p class="smaller-text"><strong>Dataset:</strong> <code>TristanBehrens/lovecraftcorpus</code></p>
                    <p class="smaller-text"><strong>Model:</strong> <a href="https://huggingface.co/theoracle/hplovecraft">theoracle/hplovecraft</a></p>
                    <p class="small-text"><strong>Learned patterns:</strong></p>
                    <ul class="smaller-text">
                        <li>Long, winding sentence structures</li>
                        <li>"Forbidden knowledge" thematic templates</li>
                        <li>References to geometry, ruins, cults, unknowable forces</li>
                        <li>First-person confessional horror narrative</li>
                        <li>Archaic diction and cosmic dread</li>
                    </ul>
                </section>

                <section>
                    <h3>Expert 2: Agatha Christie Model</h3>
                    <p class="smaller-text"><strong>Dataset:</strong> <code>realdanielbyrne/AgathaChristieText</code></p>
                    <p class="smaller-text"><strong>Model:</strong> <a href="https://huggingface.co/theoracle/agatha">theoracle/agatha</a></p>
                    <p class="small-text"><strong>Learned patterns:</strong></p>
                    <ul class="smaller-text">
                        <li>Short, clear sentence orchestration</li>
                        <li>Domestic British settings</li>
                        <li>Logical clue structuring</li>
                        <li>Psychological tension in dialogue</li>
                        <li>Golden-Age mystery pacing</li>
                    </ul>
                </section>

                <section>
                    <h3>The Orchestrator: Keyword-based Router</h3>
                    <p class="small-text"><strong>Three key advantages:</strong></p>
                    <ol class="small-text">
                        <li><strong>Interpretability:</strong> Visible, auditable selection criteria ("lovecraft", "cthulhu", "mystery", "agatha")</li>
                        <li><strong>Zero latency:</strong> No neural gating layer overhead</li>
                        <li><strong>Style purity:</strong> Only one LoRA loaded at runtime</li>
                    </ol>
                    <pre style="margin-top: 20px;"><code class="language-python" style="font-size: 0.5em;">pipe = pipeline("text-generation", model=model_name)</code></pre>
                </section>

                <section>
                    <h3>Prompt Assembly Strategy</h3>
                    <p class="small-text">Normalizes user requests for consistency:</p>
                    <ul class="smaller-text">
                        <li>Extracts "beginning" text block</li>
                        <li>Prepends unified instruction</li>
                        <li>Ensures consistent formatting</li>
                    </ul>
                    <div style="background: #f4f4f4; padding: 15px; margin-top: 15px; text-align: left;">
                        <p class="smaller-text"><strong>Example assembled prompt:</strong></p>
                        <p class="smaller-text" style="font-style: italic;">Continue the following story in the style requested:<br>
                        They sat alone in the darkened room as rain hammered...</p>
                    </div>
                </section>

                <section>
                    <h3>Why This is a Mixture-of-Experts</h3>
                    <ol class="small-text">
                        <li>Multiple specialized experts trained on disjoint corpora</li>
                        <li>Routing function selects expert based on user intent</li>
                        <li>Dynamic switching of parameter-efficient adapters</li>
                        <li>Separation of stylistic knowledge (no interference)</li>
                    </ol>
                    <p class="smaller-text" style="margin-top: 20px;"><em>Classical engineering definition of MoE: mixture controlled by external gating mechanism</em></p>
                </section>

                <section>
                    <h3>Comparison: Same Prompt, Different Styles</h3>
                    <div style="display: flex; justify-content: space-between; margin-top: 20px;">
                        <div style="width: 48%; background: #f0f0f0; padding: 15px; border: 2px solid #ccc;">
                            <p class="smaller-text"><strong>Figure 1: H.P. Lovecraft Style</strong></p>
                            <div style="background: white; height: 150px; display: flex; align-items: center; justify-content: center; margin: 10px 0;">
                                <p class="smaller-text">Generated text with<br>cosmic horror style</p>
                            </div>
                        </div>
                        <div style="width: 48%; background: #f0f0f0; padding: 15px; border: 2px solid #ccc;">
                            <p class="smaller-text"><strong>Figure 2: Agatha Christie Style</strong></p>
                            <div style="background: white; height: 150px; display: flex; align-items: center; justify-content: center; margin: 10px 0;">
                                <p class="smaller-text">Generated text with<br>mystery/detective style</p>
                            </div>
                        </div>
                    </div>
                    <p class="smaller-text" style="margin-top: 15px;"><em>Same input prompt, orchestrator selects appropriate style</em></p>
                </section>

                <section>
                    <h3>Technical Benefits</h3>
                    <ul class="small-text">
                        <li><strong>Scalability:</strong> Add new styles with new LoRA adapters + routing keywords</li>
                        <li><strong>Low compute:</strong> Gemma-2B + small LoRAs works on CPU/small GPU</li>
                        <li><strong>Style isolation:</strong> Each literary mode remains pure</li>
                        <li><strong>Maintainability:</strong> Extend without retraining</li>
                        <li><strong>User control:</strong> Force a style or automatic selection</li>
                    </ul>
                </section>

                <section>
                    <h3>Resources - Narrative Model</h3>
                    <p class="small-text"><strong>Example Jupyter Notebook:</strong></p>
                    <p><a href="https://colab.research.google.com/drive/14tMxXR3ssGLcubdp_Lh7xwPoe7dvNZNy?usp=sharing" style="font-size: 0.7em;">Google Colab</a></p>
                    <p class="small-text" style="margin-top: 20px;"><strong>Models:</strong></p>
                    <ul class="smaller-text">
                        <li><a href="https://huggingface.co/theoracle/hplovecraft">theoracle/hplovecraft</a></li>
                        <li><a href="https://huggingface.co/theoracle/agatha">theoracle/agatha</a></li>
                    </ul>
                </section>
            </section>

            <!-- Conclusions -->
            <section>
                <h2>Conclusions</h2>
                <p class="small-text"><strong>Jewellery Generation:</strong></p>
                <ul class="smaller-text">
                    <li>Reproducible pipeline from open APIs to fine-tuned models</li>
                    <li>GPT-4.1 synthetic captions for training consistency</li>
                    <li>Museum-quality output with material fidelity</li>
                </ul>
                <p class="small-text" style="margin-top: 20px;"><strong>Narrative Generation:</strong></p>
                <ul class="smaller-text">
                    <li>PEFT + modular routing achieves precise stylistic control</li>
                    <li>Clean stylistic output, low inference cost</li>
                    <li>Easily extensible MoE architecture</li>
                </ul>
            </section>

            <!-- Final Slide -->
            <section>
                <h2>Thank You</h2>
                <p class="small-text">Network Centrality Labs<br>
                For PNRR-FAIR Alma Mater University of Bologna</p>
                <p class="smaller-text" style="margin-top: 40px;">G. Monti and A. Provetti<br>
                Birkbeck, University of London</p>
            </section>

        </div>
    </div>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/reveal.js/4.5.0/reveal.min.js"></script>
    <script>
        Reveal.initialize({
            hash: true,
            controls: true,
            progress: true,
            center: true,
            transition: 'slide',
            width: 1200,
            height: 700,
            margin: 0.1
        });
    </script>
</body>
</html>